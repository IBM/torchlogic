{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1cf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall /Path/to/the/whl/file/torchlogic-0.0.1-py3-none-any.whl -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bde8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install /Path/to/the/whl/file/torchlogic-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd57348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b584d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Choices for a categorical distribution should be a tuple\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"To copy construct from a tensor, it is recommended\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"IProgress not found\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Precision is ill-defined\")\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, roc_auc_score\n",
    "\n",
    "from torchlogic.models import BanditNRNClassifier\n",
    "from torchlogic.models.mixins import ReasoningNetworkClassifierMixin\n",
    "\n",
    "from torchlogic.modules import BanditNRNModule\n",
    "from torchlogic.utils.trainers import BanditNRNTrainer\n",
    "\n",
    "from torchlogic.nn import LukasiewiczChannelOrBlock, LukasiewiczChannelAndBlock, Predicates, ConcatenateBlocksLogic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36b3c6f-5b92-4f27-a170-0389fdbfd00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want the logs from Bandit-RRN training\n",
    "\n",
    "# from carrot.logger import Logger\n",
    "\n",
    "# log_config = 'configs/logging.yaml'\n",
    "# log_dir = 'logs'\n",
    "# logger = Logger.get(log_config, log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32cc9de",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79426bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_iris()\n",
    "data.target[[10, 25, 50]]\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586f0045-0ba8-49e3-bdbd-09e895615aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_names = [\"the sepal length in cm was\", \"the sepal width in cm was\", \n",
    "                      \"the petal length in cm was\", \"the petal width in cm was\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5378c6d",
   "metadata": {},
   "source": [
    "# Prepare Bandit-NRN Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00c989",
   "metadata": {},
   "source": [
    "A dataset for the Bandit-RRN algorithm in torchlogic must return a dictionary of the following form:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'features': [N_FEATURES], 'target': [N_TARGETS], 'sample_idx': [1]\n",
    "}\n",
    "```\n",
    "\n",
    "- The `features` key contains a tensor of the features used for prediction.  Feature must be numeric and scaled between 0 and 1.\n",
    "\n",
    "- The `target` key must contain a tensor of the targets, with the values of 0 or 1 for each target.\n",
    "\n",
    "- The `sample_idx` key must contain a tensor of the row number in the data corresponding to that sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fbe56",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9863924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "X = mms.fit_transform(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63d35d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(data.target).astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b54a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "###############################################################################################################################\n",
    "# NOTE: The iris dataset is very small.  The validation set is particularly small in this case so we have enough training data.\n",
    "# For real-world applications a larger test size is likely required.\n",
    "###############################################################################################################################\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba908ac4",
   "metadata": {},
   "source": [
    "## Define PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51d41c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditNRNDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            X: np.array,\n",
    "            y: np.array\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataset suitable for BanditRRN model from torchlogic\n",
    "\n",
    "        Args:\n",
    "            X (np.array): features data scaled to [0, 1]\n",
    "            y (np.array): target data of classes 0, 1\n",
    "        \"\"\"\n",
    "        super(BanditNRNDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.sample_idx = np.arange(X.shape[0])  # index of samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.from_numpy(self.X[idx, :]).float()\n",
    "        target = torch.from_numpy(self.y[idx, :])\n",
    "        return {'features': features, 'target': target, 'sample_idx': idx}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5651ce",
   "metadata": {},
   "source": [
    "## Instantiate Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6a6c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BanditNRNDataset(X=X_train, y=y_train)\n",
    "val_dataset = BanditNRNDataset(X=X_val, y=y_val)\n",
    "test_dataset = BanditNRNDataset(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b8c779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "def create_holdout_samplers(train_dataset, pct=0.2):\n",
    "    train_size = len(train_dataset)\n",
    "    indices = list(range(train_size))\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_holdout_split_index = int(np.floor(pct * train_size))\n",
    "    train_idx, train_holdout_idx = indices[train_holdout_split_index:], indices[:train_holdout_split_index]\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    train_holdout_sampler = SubsetRandomSampler(train_holdout_idx)\n",
    "    \n",
    "    return train_sampler, train_holdout_sampler\n",
    "\n",
    "train_sampler, train_holdout_sampler = create_holdout_samplers(train_dataset)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_dataset, batch_size=32, generator=g, sampler=train_sampler,\n",
    "    pin_memory=False, persistent_workers=False, num_workers=0  # very important to optimize these settings in production\n",
    ")\n",
    "train_holdout_dl = DataLoader(\n",
    "    train_dataset, batch_size=32, generator=g, sampler=train_holdout_sampler,\n",
    "    pin_memory=False, persistent_workers=False, num_workers=0  # very important to optimize these settings in production\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_dataset, batch_size=32, generator=g, \n",
    "    pin_memory=False, persistent_workers=False, num_workers=0  # very important to optimize these settings in production\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_dataset, batch_size=32, generator=g, \n",
    "    pin_memory=False, persistent_workers=False, num_workers=0  # very important to optimize these settings in production\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd8835d-90e9-4499-9656-9218a6cd7068",
   "metadata": {},
   "source": [
    "# Build Bandit-NRN model with Domain Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba7767d4-6d18-43a7-9aba-7926f7b72c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDomainBanditNRNModule(ReasoningNetworkClassifierMixin):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_names,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        layer_sizes,\n",
    "        n_selected_features_input,\n",
    "        n_selected_features_internal,\n",
    "        n_selected_features_output,\n",
    "        perform_prune_quantile,\n",
    "        ucb_scale,\n",
    "        normal_form,\n",
    "        add_negations,\n",
    "        weight_init\n",
    "    ):\n",
    "        super(FlowerDomainBanditNRNModule, self).__init__(output_size)\n",
    "\n",
    "        # logic induction path of model\n",
    "        self.brrn = BanditNRNModule(\n",
    "            input_size=input_size,\n",
    "            output_size=output_size,\n",
    "            layer_sizes=layer_sizes,\n",
    "            feature_names=feature_names,\n",
    "            n_selected_features_input=n_selected_features_input,\n",
    "            n_selected_features_internal=n_selected_features_internal,\n",
    "            n_selected_features_output=n_selected_features_output,\n",
    "            perform_prune_quantile=perform_prune_quantile,\n",
    "            ucb_scale=ucb_scale,\n",
    "            normal_form=normal_form,\n",
    "            add_negations=add_negations,\n",
    "            weight_init=weight_init\n",
    "        )\n",
    "\n",
    "        # the model BanditNRNTrainer must have access to these layers from the extended class\n",
    "        self.model = self.brrn.model\n",
    "        self.output_layer = self.brrn.output_layer\n",
    "\n",
    "        # domain knowledge path of model\n",
    "        self.domain_rn = LukasiewiczChannelOrBlock(\n",
    "            channels=output_size,\n",
    "            in_features=input_size,\n",
    "            out_features=1,\n",
    "            n_selected_features=2,\n",
    "            parent_weights_dimension='out_features',\n",
    "            operands=Predicates(feature_names=feature_names),\n",
    "            outputs_key='domain_rn'\n",
    "        )\n",
    "\n",
    "        # feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "        # target names: ['setosa', 'versicolor', 'virginica']\n",
    "        # add the following knowledge:\n",
    "        #     class is setosa if: petal length (cm) not >= 0.16406, OR sepal width (cm) >= 0.88541\n",
    "        self.domain_rn.add_knowledge(\n",
    "            channel=0,  # corresponds to the setosa class\n",
    "            out_feature=0,  # only 1 output feature so the value must be 0\n",
    "            input_indices=[2, 1],  # corresponds to the petal length (cm) and sepal width (cm) features\n",
    "            required_thresholds=[0.16406, 0.88541],  # set the required value for the features to return a TRUE value from the logic\n",
    "            required_negations=[-1.0, 1.0],  # -1.0 indicates negated logic, while 1.0 indicates non-negated logic\n",
    "            freeze_knowledge=True  # freeze the logic such that it will receive no gradients\n",
    "        )\n",
    "\n",
    "        self.concatenate_logic = ConcatenateBlocksLogic(\n",
    "            modules=[self.brrn.output_layer, self.domain_rn],\n",
    "            outputs_key='concatenate_logic'\n",
    "        )\n",
    "\n",
    "        # NOTE: the attribute \"output_layer\" is a special attribute for the BanditRRN model.  There can be no other\n",
    "        # modules named \"output_layer\" when extending BanditRRN to add domain knowledge.\n",
    "        self.output_layer2 = LukasiewiczChannelAndBlock(\n",
    "            channels=output_size,\n",
    "            in_features=2,\n",
    "            out_features=1,\n",
    "            n_selected_features=2,\n",
    "            parent_weights_dimension='out_features',\n",
    "            operands=self.concatenate_logic,\n",
    "            outputs_key='output_layer2'\n",
    "        )\n",
    "\n",
    "        self.output_layer2.add_knowledge(\n",
    "            channel=0,  # corresponds to the setosa class\n",
    "            out_feature=0,  # only 1 output feature so the value must be 0\n",
    "            input_indices=[0, 1],  # corresponds to the concatenated logics: 0 == brrn, 1 == domain knowledge\n",
    "            required_thresholds=[0.5, 0.5],  # set the required values to influence the relative importance of each logic and the required value to return TRUE\n",
    "            required_negations=[1.0, 1.0],  # neither logic is negated\n",
    "            freeze_knowledge=True  # freeze this layer such that the relative importance and truth values receive no gradients\n",
    "        )\n",
    "\n",
    "        self.output_layer2.add_knowledge(\n",
    "            channel=1, # corresponds to the versicolor class\n",
    "            out_feature=0,  # only 1 output feature so the value must be 0\n",
    "            input_indices=[0, 1],  # corresponds to the concatenated logics: 0 == brrn, 1 == domain knowledge\n",
    "            required_thresholds=[0.5, -1.0],  # set the required truth value.  -1.0 indicates that the domain knowledge should receive 0 weight, excluding it\n",
    "            required_negations=[1.0, 1.0],  # neither logic is negated\n",
    "            freeze_knowledge=True  # freeze this layer such that the relative importance and truth values receive no gradients\n",
    "        )\n",
    "\n",
    "        self.output_layer2.add_knowledge(\n",
    "            channel=2,  # corresponds to the virginica class\n",
    "            out_feature=0,  # only 1 output feature so the value must be 0\n",
    "            input_indices=[0, 1],  # corresponds to the concatenated logics: 0 == brrn, 1 == domain knowledge\n",
    "            required_thresholds=[0.5, -1.0],  # set the required truth value.  -1.0 indicates that the domain knowledge should receive 0 weight, excluding it\n",
    "            required_negations=[1.0, 1.0],  # neither logic is negated\n",
    "            freeze_knowledge=True # freeze this layer such that the relative importance and truth values receive no gradients\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        learned_x = self.brrn(x).unsqueeze(-1).unsqueeze(-1)\n",
    "        domain_x = self.domain_rn(x)\n",
    "        x = self.concatenate_logic(learned_x, domain_x)\n",
    "        return self.output_layer2(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class FlowerDomainBanditNRNModel(BanditNRNClassifier):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_names,\n",
    "        feature_names,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        layer_sizes,\n",
    "        n_selected_features_input,\n",
    "        n_selected_features_internal,\n",
    "        n_selected_features_output,\n",
    "        perform_prune_quantile,\n",
    "        ucb_scale,\n",
    "        normal_form,\n",
    "        delta,\n",
    "        prune_strategy,\n",
    "        bootstrap,\n",
    "        swa,\n",
    "        add_negations,\n",
    "        weight_init\n",
    "    ):\n",
    "        super(FlowerDomainBanditNRNModel, self).__init__(\n",
    "            target_names=target_names,\n",
    "            feature_names=feature_names,\n",
    "            input_size=input_size,\n",
    "            output_size=output_size,\n",
    "            layer_sizes=layer_sizes,\n",
    "            n_selected_features_input=n_selected_features_input,\n",
    "            n_selected_features_internal=n_selected_features_internal,\n",
    "            n_selected_features_output=n_selected_features_output,\n",
    "            perform_prune_quantile=perform_prune_quantile,\n",
    "            ucb_scale=ucb_scale,\n",
    "            normal_form=normal_form,\n",
    "            delta=delta,\n",
    "            prune_strategy=prune_strategy,\n",
    "            bootstrap=bootstrap,\n",
    "            swa=swa,\n",
    "            add_negations=add_negations,\n",
    "            weight_init=weight_init\n",
    "        )\n",
    "\n",
    "        self.rn = FlowerDomainBanditNRNModule(\n",
    "            input_size=input_size,\n",
    "            output_size=output_size,\n",
    "            layer_sizes=layer_sizes,\n",
    "            feature_names=feature_names,\n",
    "            n_selected_features_input=n_selected_features_input,\n",
    "            n_selected_features_internal=n_selected_features_internal,\n",
    "            n_selected_features_output=n_selected_features_output,\n",
    "            perform_prune_quantile=perform_prune_quantile,\n",
    "            ucb_scale=ucb_scale,\n",
    "            normal_form=normal_form,\n",
    "            add_negations=add_negations,\n",
    "            weight_init=weight_init\n",
    "        )\n",
    "        # expose required modules in current class\n",
    "        self.set_modules(model=self.rn.brrn, root_layer=self.rn.output_layer2)\n",
    "        \n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.logger.info(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "            self.rn = nn.DataParallel(self.rn)\n",
    "        if self.USE_CUDA:\n",
    "            self.logger.info(f\"Using GPU\")\n",
    "            self.rn = self.rn.cuda()\n",
    "        elif self.USE_MPS:\n",
    "            self.logger.info(f\"Using MPS\")\n",
    "            self.rn = self.rn.to('mps')\n",
    "\n",
    "        self.USE_DATA_PARALLEL = isinstance(self.rn, torch.nn.DataParallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ca75e",
   "metadata": {},
   "source": [
    "# Train Bandit-NRN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e623836",
   "metadata": {},
   "source": [
    "## Tune Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8425fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class TuneParameters:\n",
    "    \n",
    "    def __init__(self, n_trials=10):\n",
    "        self.best_model = None\n",
    "        self.best_rn_val_performance = 0.0\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def _objective(self, trial):\n",
    "\n",
    "        ########################################################################################################################\n",
    "        # NOTE: These hyper-parameter settings are specific to the iris flower dataset.  For information on generally useful\n",
    "        # ranges of hyper-parameters and their descriptions see our documentation: \n",
    "        ########################################################################################################################\n",
    "\n",
    "        # Set Parameters\n",
    "        \n",
    "        ## Reinforced Reasoning Network Parameters\n",
    "        layer_sizes = trial.suggest_categorical('layer_sizes', [(2, ), (3, ), (5, ), (10, ), \n",
    "                                                                (2, 2), (3, 3), (5, 5), (10, 10), \n",
    "                                                                (2, 2, 2), (3, 3, 3), (5, 5, 5), (10, 10, 10)])\n",
    "        n_selected_features_input = trial.suggest_int('n_selected_features_input', low=2, high=3)\n",
    "        n_selected_features_internal = trial.suggest_int('n_selected_features_internal', low=2, high=min(3, min(layer_sizes)))\n",
    "        n_selected_features_output = trial.suggest_int('n_selected_features_output', low=2, high=min(3, layer_sizes[-1]))\n",
    "        perform_prune_plateau_count = trial.suggest_int('perform_prune_plateau_count', low=1, high=1)\n",
    "        perform_prune_quantile = trial.suggest_float('perform_prune_quantile', low=0.1, high=0.9)\n",
    "        increase_prune_plateau_count = trial.suggest_int('increase_prune_plateau_count', low=0, high=20)\n",
    "        increase_prune_plateau_count_plateau_count = trial.suggest_int('increase_prune_plateau_count_plateau_count', low=10, high=30)\n",
    "        ucb_scale = trial.suggest_float('ucb_scale', low=1.0, high=2.0)\n",
    "        normal_form = trial.suggest_categorical('normal_form', ['dnf', 'cnf'])\n",
    "        prune_strategy = trial.suggest_categorical('prune_strategy', ['class', 'logic'])\n",
    "        delta = trial.suggest_float('delta', low=2.0, high=2.0)\n",
    "        bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "        swa = trial.suggest_categorical('swa', [True, False])\n",
    "        add_negations = trial.suggest_categorical('add_negations', [True, False])\n",
    "        weight_init = trial.suggest_float('weight_init', low=0.01, high=1.0)\n",
    "\n",
    "        ## Optimizer Parameters\n",
    "\n",
    "        ### Learning Rate\n",
    "        learning_rate = trial.suggest_float('learning_rate', low=0.01, high=0.2)\n",
    "\n",
    "        ### L1 Regularization\n",
    "        use_l1 = trial.suggest_categorical('use_l1', [True, False])\n",
    "        if use_l1:\n",
    "            l1_lambda = trial.suggest_float('l1_lambda', low=0.00001, high=0.1)\n",
    "        else:\n",
    "            l1_lambda = 0\n",
    "\n",
    "        ### Weight Decay Regularization\n",
    "        use_weight_decay = trial.suggest_categorical('use_weight_decay', [True, False])\n",
    "        if use_weight_decay:\n",
    "            weight_decay = trial.suggest_float('weight_decay', low=0.00001, high=0.1)\n",
    "        else:\n",
    "            weight_decay = 0\n",
    "\n",
    "        ### Lookahead Optimization\n",
    "        use_lookahead = trial.suggest_categorical('use_lookahead', [True, False])\n",
    "        if use_lookahead:\n",
    "            lookahead_steps = trial.suggest_int('lookahead_steps', low=5, high=10, step=1)\n",
    "            lookahead_steps_size = trial.suggest_float('lookahead_steps_size', low=0.5, high=0.8)\n",
    "        else:\n",
    "            lookahead_steps = 0\n",
    "            lookahead_steps_size = 0\n",
    "\n",
    "        ### Data Augmentation\n",
    "        # augment = trial.suggest_categorical('augment', ['CM', 'MU', 'AT', None])\n",
    "        augment = trial.suggest_categorical('augment', ['CM', 'MU', None])  # excluding Adversarial Learning because it fails on Jupyter Notebooks\n",
    "        if augment is not None:\n",
    "            augment_alpha = trial.suggest_float('augment_alpha', low=0.0, high=1.0)\n",
    "        else:\n",
    "            augment_alpha = 0\n",
    "\n",
    "        ### Early Stopping\n",
    "        early_stopping_plateau_count = trial.suggest_int('early_stopping_plateau_count', low=5, high=10, step=1)\n",
    "        \n",
    "        ## Scheulder parameters\n",
    "        t_0 = trial.suggest_int('T_0', low=2, high=10, step=1)\n",
    "        t_mult = trial.suggest_int('T_mult', low=1, high=3, step=1)\n",
    "\n",
    "        # init model\n",
    "        model = FlowerDomainBanditNRNModel(\n",
    "            target_names=[x + '_label' for x in data.target_names],\n",
    "            feature_names=data.feature_names,\n",
    "            input_size=len(data.feature_names),\n",
    "            output_size=len(data.target_names),\n",
    "            layer_sizes=layer_sizes,\n",
    "            n_selected_features_input=n_selected_features_input,\n",
    "            n_selected_features_internal=n_selected_features_internal,\n",
    "            n_selected_features_output=n_selected_features_output,\n",
    "            perform_prune_quantile=perform_prune_quantile,\n",
    "            ucb_scale=ucb_scale,\n",
    "            normal_form=normal_form,\n",
    "            delta=delta,\n",
    "            prune_strategy=prune_strategy,\n",
    "            bootstrap=bootstrap,\n",
    "            swa=swa,\n",
    "            add_negations=add_negations,\n",
    "            weight_init=weight_init\n",
    "        )\n",
    "\n",
    "        epochs = 100\n",
    "        accumulation_steps = 1\n",
    "        optimizer = optim.AdamW(model.rn.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=t_0, T_mult=t_mult)\n",
    "        trainer = BanditNRNTrainer(\n",
    "            model=model,\n",
    "            loss_func=nn.BCELoss(),\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            epochs=epochs,\n",
    "            accumulation_steps=accumulation_steps,\n",
    "            l1_lambda=l1_lambda,\n",
    "            early_stopping_plateau_count=early_stopping_plateau_count,\n",
    "            perform_prune_plateau_count=perform_prune_plateau_count,\n",
    "            increase_prune_plateau_count=increase_prune_plateau_count,\n",
    "            increase_prune_plateau_count_plateau_count=increase_prune_plateau_count_plateau_count,\n",
    "            lookahead_steps=lookahead_steps,\n",
    "            lookahead_steps_size=lookahead_steps_size,\n",
    "            augment=augment,\n",
    "            augment_alpha=augment_alpha,\n",
    "            class_independent=True\n",
    "        )\n",
    "\n",
    "        # train model\n",
    "        trainer.train(train_dl, train_holdout_dl, evaluation_metric=roc_auc_score, multi_class=True)\n",
    "        trainer.set_best_state()\n",
    "\n",
    "        # evaluate model\n",
    "        predictions, targets = trainer.model.predict(val_dl)\n",
    "        rn_val_performance = trainer.model.evaluate(\n",
    "            predictions=predictions,\n",
    "            labels=targets\n",
    "        )\n",
    "\n",
    "        if rn_val_performance > self.best_rn_val_performance:\n",
    "            self.best_rn_val_performance = rn_val_performance\n",
    "            self.best_model = copy.copy(trainer.model)\n",
    "            self.best_model.rn = copy.deepcopy(trainer.model.rn)\n",
    "\n",
    "        return rn_val_performance\n",
    "    \n",
    "    def tune(self):\n",
    "        # 3. Create a study object and optimize the objective function.\n",
    "        sampler = optuna.samplers.TPESampler(multivariate=True, group=True, seed=42)\n",
    "        study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "        study.optimize(self._objective, n_trials=self.n_trials)\n",
    "        return self.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41831d70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = TuneParameters(25).tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd5a061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC:\n",
      "\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "predictions, targets = best_model.predict(val_dl)\n",
    "rn_val_performance = best_model.evaluate(\n",
    "    predictions=predictions,\n",
    "    labels=targets\n",
    ")\n",
    "class_predictions = predictions.eq(predictions.max(axis=1), axis=0).astype(int)\n",
    "predictions_probs = pd.DataFrame(softmax(predictions, axis=1), columns=data.target_names)  # CrossEntropyLoss takes logits, therefore predictions are logits\n",
    "print(\"Validation AUC:\\n\\n\", rn_val_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7529fb59-44be-41be-9507-18472cf78702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC:\n",
      "\n",
      " 0.9845679012345679\n"
     ]
    }
   ],
   "source": [
    "predictions, targets = best_model.predict(test_dl)\n",
    "rn_test_performance = best_model.evaluate(\n",
    "    predictions=predictions,\n",
    "    labels=targets\n",
    ")\n",
    "class_predictions = predictions.eq(predictions.max(axis=1), axis=0).astype(int)\n",
    "predictions_probs = pd.DataFrame(softmax(predictions, axis=1), columns=data.target_names)  # CrossEntropyLoss takes logits, therefore predictions are logits\n",
    "print(\"Test AUC:\\n\\n\", rn_test_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af3a4c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.216590</td>\n",
       "      <td>0.338428</td>\n",
       "      <td>0.444982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.372779</td>\n",
       "      <td>0.250341</td>\n",
       "      <td>0.376880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.193638</td>\n",
       "      <td>0.343143</td>\n",
       "      <td>0.463219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.216086</td>\n",
       "      <td>0.339175</td>\n",
       "      <td>0.444739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.220344</td>\n",
       "      <td>0.336452</td>\n",
       "      <td>0.443203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.337681</td>\n",
       "      <td>0.273963</td>\n",
       "      <td>0.388355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.231823</td>\n",
       "      <td>0.334332</td>\n",
       "      <td>0.433846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.208448</td>\n",
       "      <td>0.341145</td>\n",
       "      <td>0.450407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.195688</td>\n",
       "      <td>0.347636</td>\n",
       "      <td>0.456676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.222099</td>\n",
       "      <td>0.337779</td>\n",
       "      <td>0.440122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.214394</td>\n",
       "      <td>0.338687</td>\n",
       "      <td>0.446919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.312364</td>\n",
       "      <td>0.287122</td>\n",
       "      <td>0.400514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.358412</td>\n",
       "      <td>0.264022</td>\n",
       "      <td>0.377567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.318884</td>\n",
       "      <td>0.282410</td>\n",
       "      <td>0.398707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.371353</td>\n",
       "      <td>0.241776</td>\n",
       "      <td>0.386871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      setosa  versicolor  virginica\n",
       "0   0.216590    0.338428   0.444982\n",
       "1   0.372779    0.250341   0.376880\n",
       "2   0.193638    0.343143   0.463219\n",
       "3   0.216086    0.339175   0.444739\n",
       "4   0.220344    0.336452   0.443203\n",
       "5   0.337681    0.273963   0.388355\n",
       "6   0.231823    0.334332   0.433846\n",
       "7   0.208448    0.341145   0.450407\n",
       "8   0.195688    0.347636   0.456676\n",
       "9   0.222099    0.337779   0.440122\n",
       "10  0.214394    0.338687   0.446919\n",
       "11  0.312364    0.287122   0.400514\n",
       "12  0.358412    0.264022   0.377567\n",
       "13  0.318884    0.282410   0.398707\n",
       "14  0.371353    0.241776   0.386871"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e8eda1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probs_setosa</th>\n",
       "      <th>probs_versicolor</th>\n",
       "      <th>probs_virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    probs_setosa  probs_versicolor  probs_virginica\n",
       "0              0                 0                1\n",
       "1              0                 0                1\n",
       "2              0                 0                1\n",
       "3              0                 0                1\n",
       "4              0                 0                1\n",
       "5              0                 0                1\n",
       "6              0                 0                1\n",
       "7              0                 0                1\n",
       "8              0                 0                1\n",
       "9              0                 0                1\n",
       "10             0                 0                1\n",
       "11             0                 0                1\n",
       "12             0                 0                1\n",
       "13             0                 0                1\n",
       "14             0                 0                1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b127fa",
   "metadata": {},
   "source": [
    "# Inspecting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c882a",
   "metadata": {},
   "source": [
    "### Global Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9292d6",
   "metadata": {},
   "source": [
    "A global explanation prints the logic learned for each class.  The `quantile` parameter is the percent of the model you would like to be explained.\n",
    "\n",
    "We represent our features as values scaled between 0 and 1.  Therefore, we intepret the explanations to mean that large values for a particular feature represent `truthiness` of a predicate, while small values represent `falseness` of a predicate.\n",
    "\n",
    "For example, the following logic for the class `setosa`:\n",
    "\n",
    "```\n",
    "A flower is in the setosa class because: \n",
    "AND(\n",
    "    NOT(AND(\n",
    "            sepal width (cm) >= 0.77405,\n",
    "            petal length (cm) >= 0.4397)),\n",
    "    NOT(OR(\n",
    "            AND(\n",
    "            sepal width (cm) >= 0.67788,\n",
    "            petal length (cm) >= 0.20122),\n",
    "            NOT(sepal width (cm) >= 0.48579))))\n",
    "```\n",
    "\n",
    "The `logic` from above is intepreted as:\n",
    "\n",
    "```\n",
    "When BOTH of the following are true the class is \"setosa\":\n",
    "    1. The flower has a sepal width below the transformed value of 0.77, and has a petal length below the transformed value of 0.44.\n",
    "    2. The flower has a sepal width below the transformed value of 0.68 and a petal length below the transformed value of 0.20; OR the flower has a sepal width above the transformed value of 0.49.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d0d0a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A flower is in the setosa because: \n",
      "\n",
      "\n",
      "All the following are true: \n",
      "\tAny of the following are true: \n",
      "\t\tAll the following are true: \n",
      "\t\t\tIt was not true that \n",
      "\t\t\t\tThe petal width in cm was >= 0.18528\n",
      "\t\t\tIt was not true that \n",
      "\t\t\t\tThe sepal width in cm was >= 0.56848\n",
      "\t\tAll the following are true: \n",
      "\t\t\tThe sepal length in cm was >= 0.79044\n",
      "\t\t\tThe sepal width in cm was >= 0.69027\n",
      "\tAny of the following are true: \n",
      "\t\tThe sepal width in cm was >= 0.88541\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe petal length in cm was >= 0.16406\n",
      "\n",
      "A flower is in the versicolor because: \n",
      "\n",
      "\n",
      "It was not true that \n",
      "\tAll the following are true: \n",
      "\t\tThe sepal width in cm was >= 0.75596\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe sepal length in cm was >= 0.28744\n",
      "\n",
      "A flower is in the virginica because: \n",
      "\n",
      "\n",
      "Any of the following are true: \n",
      "\tIt was not true that \n",
      "\t\tIt was not true that \n",
      "\t\t\tThe sepal length in cm was >= 1.0\n",
      "\tAll the following are true: \n",
      "\t\tThe petal length in cm was >= 1.0\n",
      "\t\tThe petal width in cm was >= 1.0\n"
     ]
    }
   ],
   "source": [
    "print(best_model.explain(\n",
    "    quantile=1.0,\n",
    "    required_output_thresholds=np.array(1.0),\n",
    "    explain_type='both',\n",
    "    print_type='logical-natural', \n",
    "    explanation_prefix=\"A flower is in the\",\n",
    "    target_names=data.target_names,\n",
    "    ignore_uninformative=False,\n",
    "    rounding_precision=5,\n",
    "    # inverse_transform=mms.inverse_transform\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22edb573-c980-4ffd-85e7-87be25ca3934",
   "metadata": {},
   "source": [
    "### Did the model use our Domain Knowledge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5085a-dc8d-4307-9966-af1a00e157c9",
   "metadata": {},
   "source": [
    "We can see from the results above that the setosa class is using our logic directly.\n",
    "\n",
    "```\n",
    "A flower is in the setosa class because:\n",
    "\n",
    "    ...\n",
    "\n",
    "    ANY of the following are TRUE:\n",
    "        - NOT petal length (cm) greater than 0.164,\n",
    "        - sepal width (cm) greater than 0.885,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45b175-84f9-4f39-9404-5edce000f823",
   "metadata": {},
   "source": [
    "# Build Bandit-NRN with Uncertain Domain Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c71bc873-cfea-4502-9248-81930c0d9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDomainBanditNRNModule(ReasoningNetworkClassifierMixin):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_names,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        layer_sizes,\n",
    "        n_selected_features_input,\n",
    "        n_selected_features_internal,\n",
    "        n_selected_features_output,\n",
    "        perform_prune_quantile,\n",
    "        ucb_scale,\n",
    "        normal_form,\n",
    "        add_negations,\n",
    "        weight_init\n",
    "    ):\n",
    "        super(FlowerDomainBanditNRNModule, self).__init__(output_size)\n",
    "\n",
    "        # logic induction path of model\n",
    "        self.brrn = BanditNRNModule(\n",
    "            input_size=input_size,\n",
    "            output_size=output_size,\n",
    "            layer_sizes=layer_sizes,\n",
    "            feature_names=feature_names,\n",
    "            n_selected_features_input=n_selected_features_input,\n",
    "            n_selected_features_internal=n_selected_features_internal,\n",
    "            n_selected_features_output=n_selected_features_output,\n",
    "            perform_prune_quantile=perform_prune_quantile,\n",
    "            ucb_scale=ucb_scale,\n",
    "            normal_form=normal_form,\n",
    "            add_negations=add_negations,\n",
    "            weight_init=weight_init\n",
    "        )\n",
    "\n",
    "        # the model BanditRRNTrainer must have access to these layers from the extended class\n",
    "        self.model = self.brrn.model\n",
    "        self.output_layer = self.brrn.output_layer\n",
    "\n",
    "        # domain knowledge path of model\n",
    "        self.domain_rn = LukasiewiczChannelOrBlock(\n",
    "            channels=output_size,\n",
    "            in_features=input_size,\n",
    "            out_features=1,\n",
    "            n_selected_features=2,\n",
    "            parent_weights_dimension='out_features',\n",
    "            operands=Predicates(feature_names=feature_names),\n",
    "            outputs_key='domain_rn'\n",
    "        )\n",
    "\n",
    "        # feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "        # target names: ['setosa', 'versicolor', 'virginica']\n",
    "        # add the following knowledge:\n",
    "        #     class is setosa if: petal length (cm) not >= 0.16406, OR sepal width (cm) >= 0.88541\n",
    "        self.domain_rn.add_knowledge(\n",
    "            channel=0,  # corresponds to the setosa class\n",
    "            out_feature=0,  # only 1 output feature so the value must be 0\n",
    "            input_indices=[2, 1],  # corresponds to the petal length (cm) and sepal width (cm) features\n",
    "            required_thresholds=[0.05, 0.05],  # set the required value for the features to return a TRUE value from the logic\n",
    "            required_negations=[-1.0, 1.0],  # -1.0 indicates negated logic, while 1.0 indicates non-negated logic\n",
    "            freeze_knowledge=False  # DONT freeze the logic such that it will receive gradients\n",
    "        )\n",
    "\n",
    "        self.concatenate_logic = ConcatenateBlocksLogic(\n",
    "            modules=[self.brrn.output_layer, self.domain_rn],\n",
    "            outputs_key='concatenate_logic'\n",
    "        )\n",
    "\n",
    "        # NOTE: the attribute \"output_layer\" is a special attribute for the BanditRRN model.  There can be no other\n",
    "        # modules named \"output_layer\" when extending BanditRRN to add domain knowledge.\n",
    "        self.output_layer2 = LukasiewiczChannelAndBlock(\n",
    "            channels=output_size,\n",
    "            in_features=2,\n",
    "            out_features=1,\n",
    "            n_selected_features=2,\n",
    "            parent_weights_dimension='out_features',\n",
    "            operands=self.concatenate_logic,\n",
    "            outputs_key='output_layer2'\n",
    "        )\n",
    "\n",
    "        self.output_layer2.add_knowledge(\n",
    "            channel=0,  # corresponds to the setosa class\n",
    "            out_feature=0,  # only 1 output feature so the value must be 0\n",
    "            input_indices=[0, 1],  # corresponds to the concatenated logics: 0 == brrn, 1 == domain knowledge\n",
    "            required_thresholds=[0.5, 0.5],  # set the required values to influence the relative importance of each logic and the required value to return TRUE\n",
    "            required_negations=[1.0, 1.0],  # neither logic is negated\n",
    "            freeze_knowledge=False  # DONT freeze this layer such that the relative importance and truth values receive gradients\n",
    "        )\n",
    "\n",
    "        self.output_layer2.add_knowledge(\n",
    "            channel=1, # corresponds to the versicolor class\n",
    "            out_feature=0,  # only 1 output feature so the value must be 0\n",
    "            input_indices=[0, 1],  # corresponds to the concatenated logics: 0 == brrn, 1 == domain knowledge\n",
    "            required_thresholds=[0.5, -1.0],  # set the required truth value.  -1.0 indicates that the domain knowledge should receive 0 weight, excluding it\n",
    "            required_negations=[1.0, 1.0],  # neither logic is negated\n",
    "            freeze_knowledge=False  # DONT freeze this layer such that the relative importance and truth values receive gradients\n",
    "        )\n",
    "\n",
    "        self.output_layer2.add_knowledge(\n",
    "            channel=2,  # corresponds to the virginica class\n",
    "            out_feature=0,  # only 1 output feature so the value must be 0\n",
    "            input_indices=[0, 1],  # corresponds to the concatenated logics: 0 == brrn, 1 == domain knowledge\n",
    "            required_thresholds=[0.5, -1.0],  # set the required truth value.  -1.0 indicates that the domain knowledge should receive 0 weight, excluding it\n",
    "            required_negations=[1.0, 1.0],  # neither logic is negated\n",
    "            freeze_knowledge=False # freeze this layer such that the relative importance and truth values receive gradients\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        learned_x = self.brrn(x).unsqueeze(-1).unsqueeze(-1)\n",
    "        domain_x = self.domain_rn(x)\n",
    "        x = self.concatenate_logic(learned_x, domain_x)\n",
    "        return self.output_layer2(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class FlowerDomainBanditNRNModel(BanditNRNClassifier):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_names,\n",
    "        feature_names,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        layer_sizes,\n",
    "        n_selected_features_input,\n",
    "        n_selected_features_internal,\n",
    "        n_selected_features_output,\n",
    "        perform_prune_quantile,\n",
    "        ucb_scale,\n",
    "        normal_form,\n",
    "        delta,\n",
    "        prune_strategy,\n",
    "        bootstrap,\n",
    "        swa,\n",
    "        add_negations,\n",
    "        weight_init\n",
    "    ):\n",
    "        super(FlowerDomainBanditNRNModel, self).__init__(\n",
    "            target_names=target_names,\n",
    "            feature_names=feature_names,\n",
    "            input_size=input_size,\n",
    "            output_size=output_size,\n",
    "            layer_sizes=layer_sizes,\n",
    "            n_selected_features_input=n_selected_features_input,\n",
    "            n_selected_features_internal=n_selected_features_internal,\n",
    "            n_selected_features_output=n_selected_features_output,\n",
    "            perform_prune_quantile=perform_prune_quantile,\n",
    "            ucb_scale=ucb_scale,\n",
    "            normal_form=normal_form,\n",
    "            delta=delta,\n",
    "            prune_strategy=prune_strategy,\n",
    "            bootstrap=bootstrap,\n",
    "            swa=swa,\n",
    "            add_negations=add_negations,\n",
    "            weight_init=weight_init\n",
    "        )\n",
    "\n",
    "        self.rn = FlowerDomainBanditNRNModule(\n",
    "            input_size=input_size,\n",
    "            output_size=output_size,\n",
    "            layer_sizes=layer_sizes,\n",
    "            feature_names=feature_names,\n",
    "            n_selected_features_input=n_selected_features_input,\n",
    "            n_selected_features_internal=n_selected_features_internal,\n",
    "            n_selected_features_output=n_selected_features_output,\n",
    "            perform_prune_quantile=perform_prune_quantile,\n",
    "            ucb_scale=ucb_scale,\n",
    "            normal_form=normal_form,\n",
    "            add_negations=add_negations,\n",
    "            weight_init=weight_init\n",
    "        )   \n",
    "            \n",
    "        # expose required modules in current class\n",
    "        self.set_modules(model=self.rn.brrn, root_layer=self.rn.output_layer2)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.logger.info(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "            self.rn = nn.DataParallel(self.rn)\n",
    "        if self.USE_CUDA:\n",
    "            self.logger.info(f\"Using GPU\")\n",
    "            self.rn = self.rn.cuda()\n",
    "        elif self.USE_MPS:\n",
    "            self.logger.info(f\"Using MPS\")\n",
    "            self.rn = self.rn.to('mps')\n",
    "\n",
    "        self.USE_DATA_PARALLEL = isinstance(self.rn, torch.nn.DataParallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fcac3a-6983-4228-8f01-d94b934cc8e5",
   "metadata": {},
   "source": [
    "# Train Bandit-NRN Model to Modify Domain Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcd6dd8d-bf6a-43b1-8eb1-1dc2047d7821",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class TuneParameters:\n",
    "    \n",
    "    def __init__(self, n_trials=10):\n",
    "        self.best_model = None\n",
    "        self.best_rn_val_performance = 0.0\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def _objective(self, trial):\n",
    "\n",
    "        ########################################################################################################################\n",
    "        # NOTE: These hyper-parameter settings are specific to the iris flower dataset.  For information on generally useful\n",
    "        # ranges of hyper-parameters and their descriptions see our documentation: \n",
    "        ########################################################################################################################\n",
    "\n",
    "        # Set Parameters\n",
    "        \n",
    "        ## Reinforced Reasoning Network Parameters\n",
    "        layer_sizes = trial.suggest_categorical('layer_sizes', [(2, ), (3, ), (5, ), (10, )])\n",
    "        n_selected_features_input = trial.suggest_int('n_selected_features_input', low=2, high=3)\n",
    "        n_selected_features_internal = trial.suggest_int('n_selected_features_internal', low=2, high=min(3, min(layer_sizes)))\n",
    "        n_selected_features_output = trial.suggest_int('n_selected_features_output', low=2, high=min(3, layer_sizes[-1]))\n",
    "        perform_prune_plateau_count = trial.suggest_int('perform_prune_plateau_count', low=1, high=1)\n",
    "        perform_prune_quantile = trial.suggest_float('perform_prune_quantile', low=0.1, high=0.9)\n",
    "        increase_prune_plateau_count = trial.suggest_int('increase_prune_plateau_count', low=0, high=20)\n",
    "        increase_prune_plateau_count_plateau_count = trial.suggest_int('increase_prune_plateau_count_plateau_count', low=10, high=30)\n",
    "        ucb_scale = trial.suggest_float('ucb_scale', low=1.0, high=2.0)\n",
    "        normal_form = trial.suggest_categorical('normal_form', ['dnf', 'cnf'])\n",
    "        prune_strategy = trial.suggest_categorical('prune_strategy', ['class', 'logic'])\n",
    "        delta = trial.suggest_float('delta', low=2.0, high=2.0)\n",
    "        bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "        swa = trial.suggest_categorical('swa', [False])\n",
    "        add_negations = trial.suggest_categorical('add_negations', [True, False])\n",
    "        weight_init = trial.suggest_float('weight_init', low=0.01, high=1.0)\n",
    "\n",
    "        ## Optimizer Parameters\n",
    "\n",
    "        ### Learning Rate\n",
    "        learning_rate = trial.suggest_float('learning_rate', low=0.01, high=0.2)\n",
    "\n",
    "        ### L1 Regularization\n",
    "        use_l1 = trial.suggest_categorical('use_l1', [True, False])\n",
    "        if use_l1:\n",
    "            l1_lambda = trial.suggest_float('l1_lambda', low=0.00001, high=0.1)\n",
    "        else:\n",
    "            l1_lambda = 0\n",
    "\n",
    "        ### Weight Decay Regularization\n",
    "        use_weight_decay = trial.suggest_categorical('use_weight_decay', [True, False])\n",
    "        if use_weight_decay:\n",
    "            weight_decay = trial.suggest_float('weight_decay', low=0.00001, high=0.1)\n",
    "        else:\n",
    "            weight_decay = 0\n",
    "\n",
    "        ### Lookahead Optimization\n",
    "        use_lookahead = trial.suggest_categorical('use_lookahead', [True, False])\n",
    "        if use_lookahead:\n",
    "            lookahead_steps = trial.suggest_int('lookahead_steps', low=5, high=10, step=1)\n",
    "            lookahead_steps_size = trial.suggest_float('lookahead_steps_size', low=0.5, high=0.8)\n",
    "        else:\n",
    "            lookahead_steps = 0\n",
    "            lookahead_steps_size = 0\n",
    "\n",
    "        ### Data Augmentation\n",
    "        # augment = trial.suggest_categorical('augment', ['CM', 'MU', 'AT', None])\n",
    "        augment = trial.suggest_categorical('augment', ['CM', 'MU', None])  # excluding Adversarial Learning because it fails on Jupyter Notebooks\n",
    "        if augment is not None:\n",
    "            augment_alpha = trial.suggest_float('augment_alpha', low=0.0, high=1.0)\n",
    "        else:\n",
    "            augment_alpha = 0\n",
    "\n",
    "        ### Early Stopping\n",
    "        early_stopping_plateau_count = trial.suggest_int('early_stopping_plateau_count', low=5, high=10, step=1)\n",
    "        \n",
    "        ## Scheulder parameters\n",
    "        t_0 = trial.suggest_int('T_0', low=2, high=10, step=1)\n",
    "        t_mult = trial.suggest_int('T_mult', low=1, high=3, step=1)\n",
    "\n",
    "        # init model\n",
    "        model = FlowerDomainBanditNRNModel(\n",
    "            target_names=[x + '_label' for x in data.target_names],\n",
    "            feature_names=data.feature_names,\n",
    "            input_size=len(data.feature_names),\n",
    "            output_size=len(data.target_names),\n",
    "            layer_sizes=layer_sizes,\n",
    "            n_selected_features_input=n_selected_features_input,\n",
    "            n_selected_features_internal=n_selected_features_internal,\n",
    "            n_selected_features_output=n_selected_features_output,\n",
    "            perform_prune_quantile=perform_prune_quantile,\n",
    "            ucb_scale=ucb_scale,\n",
    "            normal_form=normal_form,\n",
    "            delta=delta,\n",
    "            prune_strategy=prune_strategy,\n",
    "            bootstrap=bootstrap,\n",
    "            swa=swa,\n",
    "            add_negations=add_negations,\n",
    "            weight_init=weight_init\n",
    "        )\n",
    "\n",
    "        epochs = 100\n",
    "        accumulation_steps = 1\n",
    "        optimizer = optim.AdamW(model.rn.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=t_0, T_mult=t_mult)\n",
    "        trainer = BanditNRNTrainer(\n",
    "            model=model,\n",
    "            loss_func=nn.BCELoss(),\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            epochs=epochs,\n",
    "            accumulation_steps=accumulation_steps,\n",
    "            l1_lambda=l1_lambda,\n",
    "            early_stopping_plateau_count=early_stopping_plateau_count,\n",
    "            perform_prune_plateau_count=perform_prune_plateau_count,\n",
    "            increase_prune_plateau_count=increase_prune_plateau_count,\n",
    "            increase_prune_plateau_count_plateau_count=increase_prune_plateau_count_plateau_count,\n",
    "            lookahead_steps=lookahead_steps,\n",
    "            lookahead_steps_size=lookahead_steps_size,\n",
    "            augment=augment,\n",
    "            augment_alpha=augment_alpha,\n",
    "            class_independent=True\n",
    "        )\n",
    "\n",
    "        # train model\n",
    "        trainer.train(train_dl, train_holdout_dl, evaluation_metric=roc_auc_score, multi_class=True)\n",
    "        trainer.set_best_state()\n",
    "\n",
    "        # evaluate model\n",
    "        predictions, targets = trainer.model.predict(val_dl)\n",
    "        rn_val_performance = trainer.model.evaluate(\n",
    "            predictions=predictions,\n",
    "            labels=targets\n",
    "        )\n",
    "\n",
    "        if rn_val_performance > self.best_rn_val_performance:\n",
    "            self.best_rn_val_performance = rn_val_performance\n",
    "            self.best_model = copy.copy(trainer.model)\n",
    "            self.best_model.rn = copy.deepcopy(trainer.model.rn)\n",
    "\n",
    "        return rn_val_performance\n",
    "    \n",
    "    def tune(self):\n",
    "        # 3. Create a study object and optimize the objective function.\n",
    "        sampler = optuna.samplers.TPESampler(multivariate=True, group=True, seed=0)\n",
    "        study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "        study.optimize(self._objective, n_trials=self.n_trials)\n",
    "        return self.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a7729-ea21-4d8d-9030-2f190eea148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = TuneParameters(25)\n",
    "best_model = tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b868457-2ccf-4630-83c1-30c4e125fd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC:\n",
      "\n",
      " 0.9940476190476191\n"
     ]
    }
   ],
   "source": [
    "predictions, targets = best_model.predict(val_dl)\n",
    "rn_val_performance = best_model.evaluate(\n",
    "    predictions=predictions,\n",
    "    labels=targets\n",
    ")\n",
    "class_predictions = predictions.eq(predictions.max(axis=1), axis=0).astype(int)\n",
    "predictions_probs = pd.DataFrame(softmax(predictions, axis=1), columns=data.target_names)\n",
    "print(\"Validation AUC:\\n\\n\", rn_val_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e53d5ea-06f7-4e19-91eb-cedca57f1c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC:\n",
      "\n",
      " 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "predictions, targets = best_model.predict(test_dl)\n",
    "rn_test_performance = best_model.evaluate(\n",
    "    predictions=predictions,\n",
    "    labels=targets\n",
    ")\n",
    "class_predictions = predictions.eq(predictions.max(axis=1), axis=0).astype(int)\n",
    "predictions_probs = pd.DataFrame(softmax(predictions, axis=1), columns=data.target_names)\n",
    "print(\"Test AUC:\\n\\n\", rn_test_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e29c2ecf-d3de-4cc2-a2d0-61ae5e7b7969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A flower is in the setosa because: \n",
      "\n",
      "\n",
      "All the following are true: \n",
      "\tAny of the following are true: \n",
      "\t\tThe sepal width in cm was >= 0.05\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe petal length in cm was >= 0.21005\n",
      "\tAny of the following are true: \n",
      "\t\tThe sepal length in cm was >= 1.0\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe sepal width in cm was >= 0.0\n",
      "\n",
      "A flower is in the versicolor because: \n",
      "\n",
      "\n",
      "All the following are true: \n",
      "\tAny of the following are true: \n",
      "\t\tThe petal length in cm was >= 1.0\n",
      "\t\tThe sepal length in cm was >= 1.0\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe petal length in cm was >= 0.0\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe sepal length in cm was >= 0.0\n",
      "\tAny of the following are true: \n",
      "\t\tThe petal width in cm was >= 1.0\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe petal width in cm was >= 0.0\n",
      "\n",
      "A flower is in the virginica because: \n",
      "\n",
      "\n",
      "All the following are true: \n",
      "\tAny of the following are true: \n",
      "\t\tThe petal length in cm was >= 1.0\n",
      "\t\tThe petal width in cm was >= 1.0\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe petal length in cm was >= 0.0\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe sepal length in cm was >= 0.0\n",
      "\tAny of the following are true: \n",
      "\t\tThe petal length in cm was >= 1.0\n",
      "\t\tThe sepal length in cm was >= 1.0\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe petal width in cm was >= 0.0\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe sepal length in cm was >= 0.0\n"
     ]
    }
   ],
   "source": [
    "print(best_model.explain(\n",
    "    quantile=1.0,\n",
    "    required_output_thresholds=np.array(1.0),\n",
    "    explain_type='both',\n",
    "    print_type='logical-natural', \n",
    "    explanation_prefix=\"A flower is in the\",\n",
    "    target_names=data.target_names,\n",
    "    ignore_uninformative=False,\n",
    "    rounding_precision=5,\n",
    "    # inverse_transform=mms.inverse_transform\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb976a64-8b87-4200-80b3-7c85288bb52e",
   "metadata": {},
   "source": [
    "### Was our domain knowledge modified?\n",
    "\n",
    "From the explanation above we can see that the model modified our logic.\n",
    "\n",
    "Our original logic that encoded our domain knowledge about the setosa flower was the following:\n",
    "\n",
    "```\n",
    "ANY of the following are TRUE:\n",
    "    - NOT petal length (cm) greater than 0.05, \n",
    "    - sepal width (cm) greater than 0.05\n",
    "```\n",
    "\n",
    "After training, the model learned to modify our logic such that it better supported the induced logic from the supervised learning stream in our Reasoning Network.\n",
    "The new modified logic is as follows.\n",
    "\n",
    "```\n",
    "ANY of the following are TRUE: \n",
    "    - NOT petal length (cm) greater than 0.21008, \n",
    "    - sepal width (cm) greater than 0.04999\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe95053-6720-458e-afcc-87e9b97b879a",
   "metadata": {},
   "source": [
    "# Sample explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dd3cbc8-63f3-4a0d-a686-4227898c5660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The flower was in the virginica because: \n",
      "\n",
      "There are several scenarios that must be met for this prediction to hold true.  The first scenario is as follows.  It was NOT true the petal length in cm was greater than or equal to 5.71806, or the petal length in cm was greater than or equal to 5.1176, or the petal width in cm was greater than or equal to 2.16408.\n",
      "\n",
      "The next scenario that must be met is as follows.  It was NOT true the sepal length in cm was greater than or equal to 7.77171, or the petal length in cm was greater than or equal to 4.3714, or the sepal length in cm was greater than or equal to 5.76421.\n",
      "\n",
      "The next scenario that must be met is as follows.  The petal length in cm was greater than or equal to 5.059, and the petal length in cm was greater than or equal to 5.13937, and it was NOT true the petal length in cm was greater than and it was NOT true equal to 5.78646\n"
     ]
    }
   ],
   "source": [
    "print(best_model.explain_samples(\n",
    "    val_dataset[0]['features'].unsqueeze(0),\n",
    "    quantile=1.0,\n",
    "    target_names=data.target_names, \n",
    "    explain_type='both',\n",
    "    sample_explanation_prefix=\"The flower was in the\",\n",
    "    print_type='natural',\n",
    "    ignore_uninformative=True,\n",
    "    rounding_precision=5,\n",
    "    inverse_transform=mms.inverse_transform\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c2e7985-00bb-4d31-a901-e23e5d1bc1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The flower was in the virginica because: \n",
      "\n",
      "\n",
      "All the following are true: \n",
      "\tIt was not true that \n",
      "\t\tAny of the following are true: \n",
      "\t\t\tThe petal length in cm was >= 5.78646\n",
      "\t\t\tIt was not true that \n",
      "\t\t\t\tThe petal length in cm was >= 5.059\n",
      "\t\t\tIt was not true that \n",
      "\t\t\t\tThe petal length in cm was >= 5.13937\n",
      "\tAny of the following are true: \n",
      "\t\tThe petal length in cm was >= 5.1176\n",
      "\t\tThe petal width in cm was >= 2.16408\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe petal length in cm was >= 5.71806\n",
      "\tAny of the following are true: \n",
      "\t\tThe petal length in cm was >= 4.3714\n",
      "\t\tThe sepal length in cm was >= 5.76421\n",
      "\t\tIt was not true that \n",
      "\t\t\tThe sepal length in cm was >= 7.77171\n"
     ]
    }
   ],
   "source": [
    "print(best_model.explain_samples(\n",
    "    val_dataset[0]['features'].unsqueeze(0),\n",
    "    quantile=1.0,\n",
    "    target_names=data.target_names, \n",
    "    explain_type='both',\n",
    "    sample_explanation_prefix=\"The flower was in the\",\n",
    "    print_type='logical-natural',\n",
    "    ignore_uninformative=True,\n",
    "    rounding_precision=5,\n",
    "    inverse_transform=mms.inverse_transform\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "965c2045-71d8-44a1-8e59-921ca539e596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The flower was in the virginica because: \n",
      "\n",
      "\n",
      "AND \n",
      "\tNOT \n",
      "\t\tOR \n",
      "\t\t\tThe petal length in cm was >= 5.78646\n",
      "\t\t\tNOT \n",
      "\t\t\t\tThe petal length in cm was >= 5.059\n",
      "\t\t\tNOT \n",
      "\t\t\t\tThe petal length in cm was >= 5.13937\n",
      "\tOR \n",
      "\t\tThe petal length in cm was >= 5.1176\n",
      "\t\tThe petal width in cm was >= 2.16408\n",
      "\t\tNOT \n",
      "\t\t\tThe petal length in cm was >= 5.71806\n",
      "\tOR \n",
      "\t\tThe petal length in cm was >= 4.3714\n",
      "\t\tThe sepal length in cm was >= 5.76421\n",
      "\t\tNOT \n",
      "\t\t\tThe sepal length in cm was >= 7.77171\n"
     ]
    }
   ],
   "source": [
    "print(best_model.explain_samples(\n",
    "    val_dataset[0]['features'].unsqueeze(0),\n",
    "    quantile=1.0,\n",
    "    target_names=data.target_names, \n",
    "    explain_type='both',\n",
    "    sample_explanation_prefix=\"The flower was in the\",\n",
    "    print_type='logical',\n",
    "    ignore_uninformative=True,\n",
    "    rounding_precision=5,\n",
    "    inverse_transform=mms.inverse_transform\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938468df-af4e-4809-ae6f-fc6b11b355e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
