{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1cf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall /Path/to/the/whl/file/torchlogic-0.0.1-py3-none-any.whl -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bde8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install /Path/to/the/whl/file/torchlogic-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd57348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e51ea9b-7071-4e4d-a4e0-b6df7a75847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! homebrew install cmake\n",
    "# ! pip install cvxpy-base\n",
    "# ! pip install aix360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b584d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Choices for a categorical distribution should be a tuple\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"To copy construct from a tensor, it is recommended\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"IProgress not found\")\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from aix360.algorithms.rbm import FeatureBinarizerFromTrees\n",
    "\n",
    "from torchlogic.models import BanditNRNRegressor\n",
    "from torchlogic.utils.trainers import BanditNRNTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73514c8-5d25-4373-9242-d23e2ee6119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want the logs from Bandit-RRN training\n",
    "\n",
    "# from carrot.logger import Logger\n",
    "\n",
    "# log_config = 'configs/logging.yaml'\n",
    "# log_dir = 'logs'\n",
    "# logger = Logger.get(log_config, log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32cc9de",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79426bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "n_samples, n_features = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35484ac5-61ec-4edb-8b94-7918248eaa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X.head(1000), y.head(1000)  # sample the data just for demonstration purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5378c6d",
   "metadata": {},
   "source": [
    "# Prepare Bandit-RRN Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00c989",
   "metadata": {},
   "source": [
    "A dataset for the Bandit-RRN algorithm in torchlogic must return a dictionary of the following form:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'features': [N_FEATURES], 'target': [N_TARGETS], 'sample_idx': [1]\n",
    "}\n",
    "```\n",
    "\n",
    "- The `features` key contains a tensor of the features used for prediction.  Feature must be numeric and scaled between 0 and 1.\n",
    "\n",
    "- The `target` key must contain a tensor of the targets, with the values of 0 or 1 for each target.\n",
    "\n",
    "- The `sample_idx` key must contain a tensor of the row number in the data corresponding to that sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fbe56",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2cf528-28d3-4b27-9c96-34268f4adacc",
   "metadata": {},
   "source": [
    "For Reasoning Networks (RNs) all values must be scaled between 0 and 1.  For a regression problem, this includes the targets.  We can use a MinMaxScaler\n",
    "for both our features and targets, separately.  Note that the RN will not make predictions below 0, or above 1.  This means, we won't make predictions below the\n",
    "minimum of our training data, or above the maximum of our training data.  If we would like to enable predictions outside the range of our training data, we could scale our values to some Min greater than 0 and Max less than 1.\n",
    "\n",
    "However, in this tutorial, we will take a different approach to transforming our input data, using FeatureBinarizationFromTrees rather than MinMaxScaler.  We detail the approach in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c15b9-a9e0-4276-8e9d-030e5eb3d32a",
   "metadata": {},
   "source": [
    "### Prepare the Feature Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe726ef-8d07-4147-baa6-aace2a8cc1e6",
   "metadata": {},
   "source": [
    "To aid in the explantions of our model, we can set our feature names to natural language that represents the values of the feature.\n",
    "In the current data, each feature represents a measurement in centimeters and is scaled between 0 to 1, so represents a percentile.\n",
    "We rename each feature to describe this represenation of our data, which is then used when extracting explanations from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d441867-41bf-4825-9a70-4c30e8f06276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
       "       'Latitude', 'Longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a29441d-b77b-4491-ae59-4845efd3940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns = ['the median income is', 'the age of the house is', 'the average number of rooms is', \n",
    "             'the average number of bedrooms is', 'the city population is',\n",
    "             'the average occupancy is', 'the latitude of the house is', \n",
    "             'the longitude of the house is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9863924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmsy = MinMaxScaler()\n",
    "y_scaled = mmsy.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b54a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.22, random_state=42)\n",
    "\n",
    "# convert to dataframes for BanditRRNDataset\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_val = pd.DataFrame(X_val, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba908ac4",
   "metadata": {},
   "source": [
    "## Define PyTorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4b1a5-2c33-42f1-b2f7-f2b3e03526f0",
   "metadata": {},
   "source": [
    "### Format binarized feature names\n",
    "\n",
    "Our FeatureBinarizationFromTrees transformation will result in non-natural language feature names.  We'll re-format the resulting feature names in order to maintain the natural language outputs we started constructing in the previous section by renaming our features.  The following function performs a formatting tranformation to the columns generated by the FeatureBinarizationFromTrees process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe4cd337-6838-4848-a4c6-ad6ce2bb785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_binarized_feature_names(binarized_column_names):\n",
    "    return list(\n",
    "        map(\n",
    "            lambda x: str(x).replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            .replace(\"/\", \" \").replace(\",\", \"\").replace(\"'\", \"\")\n",
    "            .replace(\" >= \", \" greater than or equal to \").replace(\" < \", \" less than \")\n",
    "            .replace(\" <= \", \" less than or equal to \").replace(\" > \", \" greater than \"), \n",
    "            binarized_column_names\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51d41c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditNRNDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            X: np.array,\n",
    "            y: np.array,\n",
    "            tree_num: int = None,\n",
    "            tree_depth: int = None,\n",
    "            tree_feature_selection: float = None,\n",
    "            thresh_round: int = None,\n",
    "            fbt: FeatureBinarizerFromTrees = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataset suitable for BanditRRN model from torchlogic\n",
    "\n",
    "        Args:\n",
    "            X (np.array): features data scaled to [0, 1]\n",
    "            y (np.array): target data of classes 0, 1\n",
    "        \"\"\"\n",
    "        super(BanditNRNDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.sample_idx = np.arange(X.shape[0])  # index of samples\n",
    "        self.fbt = fbt\n",
    "        self.feature_names = None\n",
    "        \n",
    "        assert isinstance(X, pd.DataFrame), \"X must be a dataframe with correct column names\"\n",
    "        \n",
    "        if self.fbt is None:\n",
    "            fit_fbt = True\n",
    "        else:\n",
    "            fit_fbt = False\n",
    "        \n",
    "        if (self.fbt is None\n",
    "            and (tree_num is not None\n",
    "                 or tree_depth is not None\n",
    "                 or tree_feature_selection is not None\n",
    "                 or thresh_round is not None)):\n",
    "            assert tree_num is not None and tree_depth is not None \\\n",
    "                and tree_feature_selection is not None and thresh_round is not None, \\\n",
    "                (\"all of 'tree_num', 'tree_depth', 'tree_feature_selection' and 'thresh_round' must be provided \"\n",
    "                 \"when using FeatureBinarizationFromTrees.\")\n",
    "            self.tree_num = tree_num\n",
    "            self.tree_depth = tree_depth\n",
    "            self.tree_feature_selection = tree_feature_selection\n",
    "            self.thresh_round = thresh_round\n",
    "            self.fbt = FeatureBinarizerFromTrees(\n",
    "                treeNum=tree_num,\n",
    "                treeDepth=tree_depth,\n",
    "                treeFeatureSelection=tree_feature_selection,\n",
    "                threshRound=thresh_round,\n",
    "                randomState=0\n",
    "            )\n",
    "\n",
    "        if self.fbt is not None:\n",
    "            numeric_columns = self.X.columns[self.X.nunique() > 2]\n",
    "            categorical_columns = self.X.columns[self.X.nunique() <= 2]\n",
    "            x_numeric = self.X[numeric_columns]\n",
    "            x_categorical = self.X[categorical_columns]\n",
    "            if fit_fbt:\n",
    "                # NOTE: to fit the tree based binarizer we need a categorical target\n",
    "                x_numeric = self.fbt.fit_transform(x_numeric, self.y > self.y.mean())\n",
    "            else:\n",
    "                x_numeric = self.fbt.transform(x_numeric)\n",
    "\n",
    "            # new feature names\n",
    "            x_numeric.columns = format_binarized_feature_names(x_numeric.columns.to_flat_index())\n",
    "\n",
    "            self.feature_names = list(x_numeric.columns) + list(categorical_columns)\n",
    "            self.X = np.hstack([x_numeric.values, x_categorical.values])\n",
    "        else:\n",
    "            self.feature_names = list(self.X.columns)\n",
    "            self.X = self.X.to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.from_numpy(self.X[idx, :]).float()\n",
    "        target = torch.from_numpy(self.y[idx, :]).float()\n",
    "        return {'features': features, 'target': target, 'sample_idx': idx}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5651ce",
   "metadata": {},
   "source": [
    "## Define helper functions for Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b8c779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_holdout_samplers(train_dataset, pct=0.2):\n",
    "    train_size = len(train_dataset)\n",
    "    indices = list(range(train_size))\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_holdout_split_index = int(np.floor(pct * train_size))\n",
    "    train_idx, train_holdout_idx = indices[train_holdout_split_index:], indices[:train_holdout_split_index]\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    train_holdout_sampler = SubsetRandomSampler(train_holdout_idx)\n",
    "    \n",
    "    return train_sampler, train_holdout_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ca75e",
   "metadata": {},
   "source": [
    "# Train Bandit-RRN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e623836",
   "metadata": {},
   "source": [
    "## Tune Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8425fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "class TuneParameters:\n",
    "    \n",
    "    def __init__(self, n_trials=10):\n",
    "        self.best_model = None\n",
    "        self.best_rn_val_performance = 1e12\n",
    "        self.n_trials = n_trials\n",
    "        self.study = None\n",
    "\n",
    "    def _objective(self, trial):\n",
    "\n",
    "        ############################################################################################################################\n",
    "        # NOTE: These hyper-parameter settings are specific to the california housing dataset.  For information on generally useful\n",
    "        # ranges of hyper-parameters and their descriptions see our documentation: \n",
    "        ############################################################################################################################\n",
    "\n",
    "        # Set Parameters\n",
    "        \n",
    "        ## Reinforced Reasoning Network Parameters\n",
    "        layer_sizes = trial.suggest_categorical('layer_sizes', [(2, ), (3, ), (5, ), (10, ), (20, ),\n",
    "                                                                (2, 2), (3, 3), (5, 5), (10, 10), (20, 20)])\n",
    "        n_selected_features_internal = trial.suggest_int('n_selected_features_internal', low=2, high=min(5, min(layer_sizes)))\n",
    "        n_selected_features_output = trial.suggest_int('n_selected_features_output', low=2, high=min(3, layer_sizes[-1]))\n",
    "        perform_prune_plateau_count = trial.suggest_int('perform_prune_plateau_count', low=1, high=8)\n",
    "        perform_prune_quantile = trial.suggest_float('perform_prune_quantile', low=0.1, high=0.9)\n",
    "        increase_prune_plateau_count = trial.suggest_int('increase_prune_plateau_count', low=0, high=20)\n",
    "        increase_prune_plateau_count_plateau_count = trial.suggest_int('increase_prune_plateau_count_plateau_count', low=10, high=30)\n",
    "        ucb_scale = trial.suggest_float('ucb_scale', low=1.5, high=2.0)\n",
    "        normal_form = trial.suggest_categorical('normal_form', ['dnf', 'cnf'])\n",
    "        prune_strategy = trial.suggest_categorical('prune_strategy', ['class', 'logic', 'logic_class'])\n",
    "        delta = trial.suggest_float('delta', low=2.0, high=2.0)\n",
    "        bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "        swa = trial.suggest_categorical('swa', [True, False])\n",
    "        add_negations = trial.suggest_categorical('add_negations', [False])\n",
    "        weight_init = trial.suggest_float('weight_init', low=0.01, high=1.0)\n",
    "\n",
    "        ## Optimizer Parameters\n",
    "\n",
    "        ### Learning Rate\n",
    "        learning_rate = trial.suggest_float('learning_rate', low=0.01, high=0.5)\n",
    "\n",
    "        ### Weight Decay Regularization\n",
    "        use_weight_decay = trial.suggest_categorical('use_weight_decay', [True, False])\n",
    "        if use_weight_decay:\n",
    "            weight_decay = trial.suggest_float('weight_decay', low=0.00001, high=0.1)\n",
    "        else:\n",
    "            weight_decay = 0\n",
    "\n",
    "        ### Lookahead Optimization\n",
    "        use_lookahead = trial.suggest_categorical('use_lookahead', [True, False])\n",
    "        if use_lookahead:\n",
    "            lookahead_steps = trial.suggest_int('lookahead_steps', low=5, high=10, step=1)\n",
    "            lookahead_steps_size = trial.suggest_float('lookahead_steps_size', low=0.5, high=0.8)\n",
    "        else:\n",
    "            lookahead_steps = 0\n",
    "            lookahead_steps_size = 0\n",
    "\n",
    "        ### Data Augmentation\n",
    "        ### Only AT data augmentation is applicable for regression tasks and it does not work in Jupyter Notebooks\n",
    "        augment = None\n",
    "        augment_alpha = 0\n",
    "        # # augment = trial.suggest_categorical('augment', ['CM', 'MU', 'AT', None])\n",
    "        # if augment is not None:\n",
    "        #     augment_alpha = trial.suggest_float('augment_alpha', low=0.0, high=1.0)\n",
    "        # else:\n",
    "        #     augment_alpha = 0\n",
    "\n",
    "        ### Early Stopping\n",
    "        early_stopping_plateau_count = trial.suggest_int('early_stopping_plateau_count', low=20, high=50, step=1)\n",
    "        \n",
    "        ## Scheulder parameters\n",
    "        t_0 = trial.suggest_int('T_0', low=2, high=10, step=1)\n",
    "        t_mult = trial.suggest_int('T_mult', low=1, high=3, step=1)\n",
    "\n",
    "        ### Tree Binarization\n",
    "        if add_negations:\n",
    "            use_fbt = False\n",
    "        else:\n",
    "            use_fbt = trial.suggest_categorical('use_fbt', [True])\n",
    "        if use_fbt:\n",
    "            tree_num = trial.suggest_int(\"fbt_tree_num\", low=2, high=20)\n",
    "            tree_depth = trial.suggest_int(\"fbt_tree_depth\", low=2, high=10)\n",
    "            tree_feature_selection = trial.suggest_float(\"fbt_tree_feature_selection\", low=0.3, high=1.0)\n",
    "            thresh_round = trial.suggest_int(\"fbt_thresh_round\", low=0, high=3)\n",
    "            train_dataset = BanditNRNDataset(\n",
    "                X=X_train, \n",
    "                y=y_train,\n",
    "                tree_num=tree_num,\n",
    "                tree_depth=tree_depth,\n",
    "                tree_feature_selection=tree_feature_selection,\n",
    "                thresh_round=thresh_round\n",
    "            )\n",
    "            val_dataset = BanditNRNDataset(X=X_val, y=y_val, fbt=train_dataset.fbt)\n",
    "        else:\n",
    "            train_dataset = BanditNRNDataset(X=X_train, y=y_train)\n",
    "            val_dataset = BanditNRNDataset(X=X_val, y=y_val)\n",
    "\n",
    "        # NOTE: Tree Binarizataion changes the number of features so input feature selection must be performed after that process\n",
    "        n_selected_features_input = trial.suggest_int('n_selected_features_input', low=2, high=min(15, len(train_dataset.feature_names)))\n",
    "\n",
    "        train_sampler, train_holdout_sampler = create_holdout_samplers(train_dataset)\n",
    "\n",
    "        train_dl = DataLoader(\n",
    "            train_dataset, batch_size=32, generator=g, sampler=train_sampler,\n",
    "            pin_memory=False, persistent_workers=False, num_workers=0  # very important to optimize these settings in production\n",
    "        )\n",
    "        train_holdout_dl = DataLoader(\n",
    "            train_dataset, batch_size=32, generator=g, sampler=train_holdout_sampler,\n",
    "            pin_memory=False, persistent_workers=False, num_workers=0  # very important to optimize these settings in production\n",
    "        )\n",
    "        val_dl = DataLoader(\n",
    "            val_dataset, batch_size=32, generator=g, \n",
    "            pin_memory=False, persistent_workers=False, num_workers=0  # very important to optimize these settings in production\n",
    "        )\n",
    "            \n",
    "        # init model\n",
    "        model = BanditNRNRegressor(\n",
    "            target_names='price_label',\n",
    "            feature_names=train_dataset.feature_names,\n",
    "            input_size=len(train_dataset.feature_names),\n",
    "            layer_sizes=layer_sizes,\n",
    "            n_selected_features_input=n_selected_features_input,\n",
    "            n_selected_features_internal=n_selected_features_internal,\n",
    "            n_selected_features_output=n_selected_features_output,\n",
    "            perform_prune_quantile=perform_prune_quantile,\n",
    "            ucb_scale=ucb_scale,\n",
    "            normal_form=normal_form,\n",
    "            delta=delta,\n",
    "            prune_strategy=prune_strategy,\n",
    "            bootstrap=bootstrap,\n",
    "            swa=swa,\n",
    "            add_negations=add_negations\n",
    "        )\n",
    "\n",
    "        epochs = 200\n",
    "        accumulation_steps = 1\n",
    "        optimizer = optim.Adam(model.rn.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=t_0, T_mult=t_mult)\n",
    "        trainer = BanditNRNTrainer(\n",
    "            model=model,\n",
    "            loss_func=nn.MSELoss(),\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            epochs=epochs,\n",
    "            accumulation_steps=accumulation_steps,\n",
    "            early_stopping_plateau_count=early_stopping_plateau_count,\n",
    "            perform_prune_plateau_count=perform_prune_plateau_count,\n",
    "            increase_prune_plateau_count=increase_prune_plateau_count,\n",
    "            increase_prune_plateau_count_plateau_count=increase_prune_plateau_count_plateau_count,\n",
    "            lookahead_steps=lookahead_steps,\n",
    "            lookahead_steps_size=lookahead_steps_size,\n",
    "            augment=augment,\n",
    "            augment_alpha=augment_alpha,\n",
    "            objective='minimize'  # Must pass minimize objective since we are using mean_squared_error as our performance metric\n",
    "        )\n",
    "\n",
    "        # train model\n",
    "        # The trainer defaults to optimizing the validation roc_auc_score.  To optimize against a different metric pass the sklearn metric to the 'evaluation_metric' parameter\n",
    "        trainer.train(train_dl, train_holdout_dl, evaluation_metric=mean_squared_error, multi_class=True)\n",
    "        trainer.set_best_state()\n",
    "\n",
    "        # evaluate model\n",
    "        predictions, targets = trainer.model.predict(val_dl)\n",
    "        rn_val_performance = trainer.model.evaluate(\n",
    "            predictions=predictions,\n",
    "            labels=targets,\n",
    "            output_metric=mean_squared_error\n",
    "        )\n",
    "\n",
    "        if rn_val_performance < self.best_rn_val_performance:\n",
    "            self.best_rn_val_performance = rn_val_performance\n",
    "            self.best_model = copy.copy(trainer.model)\n",
    "            self.best_model.rn = copy.deepcopy(trainer.model.rn)\n",
    "\n",
    "        return rn_val_performance\n",
    "    \n",
    "    def tune(self):\n",
    "        # 3. Create a study object and optimize the objective function.\n",
    "        sampler = optuna.samplers.TPESampler(multivariate=True, group=True, seed=42)\n",
    "        self.study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "        self.study.optimize(self._objective, n_trials=self.n_trials)\n",
    "        return self.best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41831d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = TuneParameters(2)\n",
    "best_model = tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "855f171f-eecb-4bc7-8d49-1cdbefbf5c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tree Binarization\n",
    "if 'use_fbt' in tuner.study.best_params and tuner.study.best_params['use_fbt']:\n",
    "    train_dataset = BanditNRNDataset(\n",
    "        X=X_train, \n",
    "        y=y_train,\n",
    "        tree_num=tuner.study.best_params['fbt_tree_num'],\n",
    "        tree_depth=tuner.study.best_params['fbt_tree_depth'],\n",
    "        tree_feature_selection=tuner.study.best_params['fbt_tree_feature_selection'],\n",
    "        thresh_round=tuner.study.best_params['fbt_thresh_round']\n",
    "    )\n",
    "    val_dataset = BanditNRNDataset(X=X_val, y=y_val, fbt=train_dataset.fbt)\n",
    "    test_dataset = BanditNRNDataset(X=X_test, y=y_test, fbt=train_dataset.fbt)\n",
    "else:\n",
    "    train_dataset = BanditNRNDataset(X=X_train, y=y_train)\n",
    "    val_dataset = BanditNRNDataset(X=X_val, y=y_val)\n",
    "    test_dataset = BanditNRNDataset(X=X_test, y=y_test)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_dataset, batch_size=32, generator=g, shuffle=False,\n",
    "    pin_memory=False, persistent_workers=False, num_workers=0  # very important to optimize these settings in production\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_dataset, batch_size=32, generator=g, shuffle=False,\n",
    "    pin_memory=False, persistent_workers=False, num_workers=0  # very important to optimize these settings in production\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_dataset, batch_size=32, generator=g, shuffle=False,\n",
    "    pin_memory=False, persistent_workers=False, num_workers=0  # very important to optimize these settings in production\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cd5a061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE:\n",
      "\n",
      " 0.02023284323513508\n",
      "\n",
      "\n",
      "Validation R^2:\n",
      "\n",
      " 0.37921359875430616\n"
     ]
    }
   ],
   "source": [
    "predictions, targets = best_model.predict(val_dl)\n",
    "# Evaluate defaults to compute AUC\n",
    "rn_val_performance = best_model.evaluate(\n",
    "    predictions=predictions,\n",
    "    labels=targets,\n",
    "    multi_class=True,\n",
    "    output_metric=mean_squared_error\n",
    ")\n",
    "print(\"Validation MSE:\\n\\n\", rn_val_performance)\n",
    "\n",
    "# Evaluate with a different metric\n",
    "rn_val_performance = best_model.evaluate(\n",
    "    predictions=predictions,\n",
    "    labels=targets,\n",
    "    output_metric=r2_score,\n",
    "    multi_class=True\n",
    ")\n",
    "print(\"\\n\\nValidation R^2:\\n\\n\", rn_val_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7529fb59-44be-41be-9507-18472cf78702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE:\n",
      "\n",
      " 0.028111455962061882\n",
      "\n",
      "\n",
      "Test R^2:\n",
      "\n",
      " 0.2529919894496321\n"
     ]
    }
   ],
   "source": [
    "predictions, targets = best_model.predict(test_dl)\n",
    "# Evaluate defaults to compute AUC\n",
    "rn_test_performance = best_model.evaluate(\n",
    "    predictions=predictions,\n",
    "    labels=targets,\n",
    "    multi_class=True,\n",
    "    output_metric=mean_squared_error\n",
    ")\n",
    "print(\"Test MSE:\\n\\n\", rn_test_performance)\n",
    "\n",
    "# Evaluate with a different metric\n",
    "rn_val_performance = best_model.evaluate(\n",
    "    predictions=predictions,\n",
    "    labels=targets,\n",
    "    output_metric=r2_score,\n",
    "    multi_class=True\n",
    ")\n",
    "print(\"\\n\\nTest R^2:\\n\\n\", rn_val_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c23edd-caf0-439f-85ec-91e1cbf1d489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'probs_price'}>]], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b127fa",
   "metadata": {},
   "source": [
    "# Inspecting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c882a",
   "metadata": {},
   "source": [
    "### Global Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9292d6",
   "metadata": {},
   "source": [
    "A global explanation prints the logic learned for each class.  The `quantile` parameter is the percent of the model you would like to be explained.\n",
    "\n",
    "In this tutorial we used a data transformation technique called FeatureBinarizationFromTrees that uses a tree-based model to binarize continuous valued features into boolean features.  For example, we may create two features during this process that correspond to \"X >= 3\", \"X < 3\", \"X >= 1.5\", \"X < 1.5\", each of which takes values 0 or 1.  In addition, our categorical features have been dummy encoded, and therefore also take the values 0 or 1.\n",
    "\n",
    "In this scenario, the bounds learned in our Reasoning Network can effectively be ignored for many use cases (not to say they are not informative!). For this reason, we have set `show_bounds=False` in the explanations below because the features themselves include bounds on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model.explain(\n",
    "    quantile=1.0,\n",
    "    required_output_thresholds=torch.tensor(.5),\n",
    "    explain_type='both',\n",
    "    print_type='natural', \n",
    "    explanation_prefix=\"A house has a\",\n",
    "    target_names=['price'],\n",
    "    ignore_uninformative=True,\n",
    "    rounding_precision=3,\n",
    "    show_bounds=False,\n",
    "    inverse_transform_target=mmsy.inverse_transform\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6cfe1ba-4598-45f0-ad4a-266923ba4972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A house has a predicted price of 2.8 because: \n",
      "\n",
      "In various scenarios, this prediction could prove to be correct.  The first scenario that could be met is as follows.  It was NOT true the average number of rooms is less than or equal to 5.5, and the average occupancy is less than or equal to 3.4.\n",
      "\n",
      "The next scenario that could be met is as follows.  The age of the house is less than or equal to 28.5, and the median income is greater than 5.7\n"
     ]
    }
   ],
   "source": [
    "print(best_model.explain(\n",
    "    quantile=1.0,\n",
    "    required_output_thresholds=torch.tensor(.5),\n",
    "    explain_type='both',\n",
    "    print_type='natural', \n",
    "    explanation_prefix=\"A house has a\",\n",
    "    target_names=['price'],\n",
    "    ignore_uninformative=True,\n",
    "    rounding_precision=3,\n",
    "    show_bounds=False,\n",
    "    inverse_transform_target=mmsy.inverse_transform\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a8229",
   "metadata": {},
   "source": [
    "### Sample Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e9d6fa",
   "metadata": {},
   "source": [
    "A sample explanation prints the logic that was used for a particular sample's predition. The `quantile` parameter is the percent of the model you would like to be explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d44227d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a0edc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The house has a a predicted price of 2.63149 because: \n",
      "\n",
      "\n",
      "All the following are true: \n",
      "\tThe age of the house was between 28.5 and 47\n",
      "\tThe average occupancy was greater than 3.4\n"
     ]
    }
   ],
   "source": [
    "print(best_model.explain_samples(\n",
    "    val_dataset[idx]['features'].unsqueeze(0), \n",
    "    quantile=1.0,\n",
    "    target_names=['price'], \n",
    "    explain_type='both',\n",
    "    sample_explanation_prefix=\"The house has a\",\n",
    "    print_type='logical-natural',\n",
    "    ignore_uninformative=False,\n",
    "    rounding_precision=5,\n",
    "    # inverse_transform_features=mms.inverse_transform,\n",
    "    show_bounds=False,\n",
    "    inverse_transform_target=mmsy.inverse_transform,\n",
    "    simplify=True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a47d5d",
   "metadata": {},
   "source": [
    "### Printing the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80350a16",
   "metadata": {},
   "source": [
    "We can inspect the weights the model learned for each logic to see how important they are to the overall prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb1326c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REASONING NETWORK MODEL FOR: price\n",
      "Logic at depth 2: the age of the house is greater than 47.0 >= 1.0\n",
      "output: tensor([0., 0.])\n",
      "\n",
      "Logic at depth 1: ['NOT(the age of the house is greater than 47.0 >= 1.0)']\n",
      "weights: tensor([-0.0309,  0.0453])\n",
      "output: 0.04531592130661011\n",
      "required_threshold: 0.11905547231435776\n",
      "\n",
      "Logic at depth 2: NOT(the age of the house is less than or equal to 28.5 >= 0.13877)\n",
      "output: tensor([1., 0.])\n",
      "\n",
      "Logic at depth 1: ['NOT(the age of the house is less than or equal to 28.5 >= 0.13877)']\n",
      "weights: tensor([0.4719, 0.1215])\n",
      "output: 0.8784533739089966\n",
      "required_threshold: 0.8953201770782471\n",
      "\n",
      "Logic at depth 2: NOT(the average occupancy is less than or equal to 3.4 >= 0.06143)\n",
      "output: tensor([0., 0.])\n",
      "\n",
      "Logic at depth 1: ['NOT(the average occupancy is less than or equal to 3.4 >= 0.06143)']\n",
      "weights: tensor([ 0.2560, -0.3503])\n",
      "output: 0.743966281414032\n",
      "required_threshold: 0.759693443775177\n",
      "\n",
      "Logic at depth 0: ['OR(NOT(the age of the house is greater than 47.0 >= 1.0), NOT(the age of the house is less than or equal to 28.5 >= 0.13877), NOT(the average occupancy is less than or equal to 3.4 >= 0.06143))']\n",
      "weights: tensor([-0.0626,  0.2737,  0.2936])\n",
      "output: 0.4617008566856384\n",
      "required_threshold: 0.4663178622722626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model.print_samples(\n",
    "    val_dataset[idx]['features'].unsqueeze(0),\n",
    "    quantile=1.0,\n",
    "    target_names=['price'], \n",
    "    explain_type='both',\n",
    "    print_type='logical',\n",
    "    ignore_uninformative=False,\n",
    "    rounding_precision=5,\n",
    "    # inverse_transform=mms.inverse_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf22a50e-ac0d-4813-8188-562cf1225680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REASONING NETWORK MODEL FOR: price\n",
      "Logic at depth 2: \n",
      "output: tensor([1., 0.])\n",
      "\n",
      "Logic at depth 2: the age of the house is less than or equal to 28.5 >= 1.0, the median income is greater than 5.7 >= 1.0\n",
      "output: tensor([1., 1.])\n",
      "\n",
      "Logic at depth 1: ['AND(the age of the house is less than or equal to 28.5 >= 1.0, the median income is greater than 5.7 >= 1.0)']\n",
      "weights: tensor([0.4719, 0.1215])\n",
      "output: 1.0\n",
      "required_threshold: 1.0\n",
      "\n",
      "Logic at depth 2: NOT(the average number of rooms is less than or equal to 5.5 >= 0.0), the average occupancy is less than or equal to 3.4 >= 1.0\n",
      "output: tensor([1., 0.])\n",
      "\n",
      "Logic at depth 1: ['AND(NOT(the average number of rooms is less than or equal to 5.5 >= 0.0), the average occupancy is less than or equal to 3.4 >= 1.0)']\n",
      "weights: tensor([ 0.2560, -0.3503])\n",
      "output: 1.0\n",
      "required_threshold: 1.0\n",
      "\n",
      "Logic at depth 0: ['OR(AND(NOT(the average number of rooms is less than or equal to 5.5 >= 0.0), the average occupancy is less than or equal to 3.4 >= 1.0), AND(the age of the house is less than or equal to 28.5 >= 1.0, the median income is greater than 5.7 >= 1.0))']\n",
      "weights: tensor([-0.0626,  0.2737,  0.2936])\n",
      "output: 0.699999988079071\n",
      "required_threshold: 0.699999988079071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model.print(\n",
    "    quantile=1.0,\n",
    "    required_output_thresholds=torch.tensor(0.7),\n",
    "    explain_type='both',\n",
    "    print_type='logical', \n",
    "    target_names=['price'],\n",
    "    ignore_uninformative=False,\n",
    "    rounding_precision=5,\n",
    "    # inverse_transform=mms.inverse_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d24d5-6094-42bc-a3bf-d5bfe3a7547e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
